{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2896779",
   "metadata": {},
   "source": [
    "# Prior-Anchored Ridge Logistic Regression for Distress Prediction\n",
    "This section implements a prior-anchored ridge logistic regression, using published CHS coefficients as a strong prior. Instead of hard-coding coefficients, we shrink our estimates toward the CHS values, allowing local recalibration while borrowing strength from the published model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138d9559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score\n",
    "\n",
    "def fit_ridge_with_prior(X, y, beta_chs, lambdas, cv_folds=5, scoring='log_loss'):\n",
    "    # Center features and add intercept\n",
    "    X = (X - np.mean(X, axis=0))\n",
    "    best_score = None\n",
    "    best_beta = None\n",
    "    best_lam = None\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "    for lam in lambdas:\n",
    "        # Custom penalty: ||beta - beta_chs||^2\n",
    "        # Use LogisticRegression with L2, then shift solution toward beta_chs\n",
    "        clf = LogisticRegression(penalty='l2', C=1/lam, fit_intercept=True, solver='lbfgs', max_iter=1000)\n",
    "        scores = []\n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            clf.fit(X[train_idx], y[train_idx])\n",
    "            beta_hat = clf.coef_.flatten()\n",
    "            # Shrink toward beta_chs\n",
    "            beta = (beta_hat + lam * beta_chs) / (1 + lam)\n",
    "            y_pred = clf.predict_proba(X[test_idx])[:,1]\n",
    "            if scoring == 'log_loss':\n",
    "                score = log_loss(y[test_idx], y_pred)\n",
    "            elif scoring == 'brier':\n",
    "                score = brier_score_loss(y[test_idx], y_pred)\n",
    "            elif scoring == 'auc':\n",
    "                score = roc_auc_score(y[test_idx], y_pred)\n",
    "            scores.append(score)\n",
    "        avg_score = np.mean(scores)\n",
    "        if best_score is None or avg_score < best_score:\n",
    "            best_score = avg_score\n",
    "            best_beta = beta\n",
    "            best_lam = lam\n",
    "    return best_beta, best_lam, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d16552",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "Assuming you have standardized predictors `X`, binary labels `y`, and CHS coefficients `beta_chs` in the same order, run:\n",
    "```\n",
    "This will estimate coefficients that are shrunk toward the CHS prior, with the amount of shrinkage determined by cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a8a58",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "- For rare events, consider Firth-penalized logistic regression and class weighting.\n",
    "- For full Bayesian inference, place a normal prior centered at CHS coefficients and tune the prior variance by time-series CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8165fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and Data Loading ---\n",
    "import sf_quant.data as sfd\n",
    "import sf_quant.optimizer as sfo\n",
    "import sf_quant.backtester as sfb\n",
    "import sf_quant.performance as sfp\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- Local data directory (BoQ Files) ---\n",
    "BOQ_DIR = r\"/Users/danilomcmaster/Desktop/ACME/BoQ Files\"\n",
    "\n",
    "# --- Load CRSP and Compustat Data ---\n",
    "crsp_pricing = pl.read_csv(\n",
    "    f\"{BOQ_DIR}/crsp_pricing.csv\",\n",
    "    infer_schema_length=10000,\n",
    "    schema_overrides={\"CUSIP\": pl.Utf8, \"NCUSIP\": pl.Utf8}\n",
    ")\n",
    "crsp_delisting = pl.read_csv(\n",
    "    f\"{BOQ_DIR}/crsp_delisting.csv\",\n",
    "    infer_schema_length=10000,\n",
    "    schema_overrides={\"CUSIP\": pl.Utf8, \"NCUSIP\": pl.Utf8}\n",
    ")\n",
    "crsp = crsp_pricing.join(crsp_delisting, on=['date', 'PERMNO', 'CUSIP'])\n",
    "compustat = pl.read_csv(\n",
    "    f\"{BOQ_DIR}/compustat_updated.csv\",\n",
    "    infer_schema_length=10000,\n",
    "    schema_overrides={\"GVKEY\": pl.Utf8, \"CUSIP\": pl.Utf8, \"TIC\": pl.Utf8, \"CONM\": pl.Utf8}\n",
    ")\n",
    "link = pl.read_csv(f\"{BOQ_DIR}/link.csv\")\n",
    "link = link.rename({'LPERMNO': 'PERMNO', 'GVKEY': 'gvkey'})\n",
    "compustat = compustat.join(link, on=['gvkey', 'cusip'])\n",
    "compustat = compustat.rename({'datadate': 'date'})\n",
    "data = crsp.join(compustat, on=['PERMNO', 'date'], how='left')\n",
    "\n",
    "# --- Load SP500 Data ---\n",
    "sp500 = pl.read_csv(f\"{BOQ_DIR}/sp500.csv\")\n",
    "sp500 = sp500.with_columns(\n",
    "    pl.col(\"caldt\").str.strptime(pl.Date, format=\"%Y-%m-%d\").alias(\"caldt_date\")\n",
    ")\n",
    "sp500 = sp500.with_columns(\n",
    "    pl.col(\"caldt_date\").dt.month_end().alias(\"date\")\n",
    ")\n",
    "# ensure `data.date` is a proper polars Date before joining\n",
    "data = data.with_columns(\n",
    "    pl.col(\"date\").str.strptime(pl.Date, format=\"%Y-%m-%d\").alias(\"date\")\n",
    ")\n",
    "data = data.join(sp500, on='date')\n",
    "\n",
    "# --- Signal Construction, Cleaning, and Feature Engineering ---\n",
    "# (Add all feature engineering, winsorization, target variable, and modeling code from distress_research_1.ipynb)\n",
    "# ...\n",
    "# For brevity, you may want to copy the full code blocks for:\n",
    "# - Feature engineering (beq, meq, mbq, taq_adjusted, rsize, exret, nimta, tlmta, cshmta, nita, tlta, sigma, nimtaavg, exretavg)\n",
    "# - Winsorization\n",
    "# - Target variable (failure)\n",
    "# - Model fitting (logit, ridge, etc.)\n",
    "# - Portfolio formation and backtesting\n",
    "#\n",
    "# Example: (see distress_research_1.ipynb for full code)\n",
    "# data = data.with_columns(...)\n",
    "# in_sample = data.filter(...)\n",
    "# predictors = [...]\n",
    "# reg_data = in_sample.select([...]).drop_nulls().to_pandas()\n",
    "# X = reg_data[predictors]\n",
    "# y = reg_data[\"failure\"]\n",
    "# X = sm.add_constant(X)\n",
    "# logit_model = sm.Logit(y, X)\n",
    "# result = logit_model.fit()\n",
    "# print(result.summary())\n",
    "# ...\n",
    "# (Continue with all relevant code for your workflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c861f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crsp_pricing loaded, rows= 4709982 cols= 12\n"
     ]
    }
   ],
   "source": [
    "# Quick path check: read one CSV and print rows/columns\n",
    "import polars as pl\n",
    "BOQ_DIR = r\"/Users/danilomcmaster/Desktop/ACME/BoQ Files\"\n",
    "try:\n",
    "    df = pl.read_csv(f\"{BOQ_DIR}/crsp_pricing.csv\", infer_schema_length=1000)\n",
    "    print('crsp_pricing loaded, rows=', df.height, 'cols=', df.width)\n",
    "except Exception as e:\n",
    "    print('Failed to load crsp_pricing.csv:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12df6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lam 0.01 cv_log_loss 0.018377152276795657\n",
      "lam 0.046415888336127774 cv_log_loss 0.018426520911867424\n",
      "lam 0.21544346900318834 cv_log_loss 0.018726051080883124\n",
      "lam 1.0 cv_log_loss 0.01994551625790188\n",
      "lam 4.6415888336127775 cv_log_loss 0.02151300373599037\n",
      "lam 21.54434690031882 cv_log_loss 0.022256588222051413\n",
      "lam 100.0 cv_log_loss 0.022349798803841435\n",
      "Selected lambda: 0.01\n",
      "Assigned p_failure_12_ridge to in_sample; non-null count= 2076366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/4257833063.py:92: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  orig = df.with_row_count('_idx')\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/4257833063.py:96: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  in_sample = in_sample.with_row_count('_idx')\n"
     ]
    }
   ],
   "source": [
    "# Fit prior-anchored ridge logistic and assign probabilities to `in_sample`\n",
    "# Requires: `in_sample` Polars frame present in the notebook (with predictors and failure_12 target)\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# --- Settings ---\n",
    "predictors = ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "\n",
    "# CHS 12-month coefficients (prior)\n",
    "beta_chs_dict = {\n",
    "    'nimtaavg_w': -20.26,\n",
    "    'tlmta_w': 1.42,\n",
    "    'rsize_w': -0.55,\n",
    "    'exretavg_w': -0.999,\n",
    "    'sigma_w': 0.317,\n",
    "    'log_prc_w': 0.953,\n",
    "    'cshmta_w': -0.097,\n",
    "    'mbq_w': 0.075\n",
    "}\n",
    "\n",
    "beta_chs = np.array([beta_chs_dict.get(p, 0.0) for p in predictors])\n",
    "\n",
    "# Work on a local copy to avoid mutating original in_sample unintentionally\n",
    "df = in_sample.select(['failure_12'] + predictors)\n",
    "# median-impute predictor nulls\n",
    "for p in predictors:\n",
    "    if p in df.columns:\n",
    "        median_val = df.select(pl.col(p)).drop_nulls().median().to_numpy()[0] if df.select(pl.col(p)).drop_nulls().height>0 else 0.0\n",
    "        df = df.with_columns(pl.col(p).fill_null(median_val))\n",
    "# drop rows where failure_12 is null\n",
    "if 'failure_12' in df.columns:\n",
    "    df = df.filter(pl.col('failure_12').is_not_null())\n",
    "\n",
    "if df.height == 0:\n",
    "    raise ValueError('No rows available after preparing predictors/target -- ensure feature engineering ran')\n",
    "\n",
    "y = np.asarray(df['failure_12'].to_numpy())\n",
    "X = np.column_stack([np.asarray(df[p].to_numpy()) for p in predictors])\n",
    "\n",
    "# Cross-validated selection of lambda via TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "lambdas = np.logspace(-2, 2, 7)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "best_score = np.inf\n",
    "best_lam = None\n",
    "best_beta = None\n",
    "\n",
    "Xc = X - X.mean(axis=0)\n",
    "for lam in lambdas:\n",
    "    scores = []\n",
    "    for train_idx, test_idx in tscv.split(Xc):\n",
    "        # require both classes in training fold\n",
    "        if len(np.unique(y[train_idx])) < 2:\n",
    "            scores.append(np.inf)\n",
    "            continue\n",
    "        clf = LogisticRegression(penalty='l2', C=1/lam, solver='lbfgs', max_iter=1000)\n",
    "        clf.fit(Xc[train_idx], y[train_idx])\n",
    "        beta_hat = clf.coef_.ravel()\n",
    "        beta_post = (beta_hat + lam * beta_chs) / (1 + lam)\n",
    "        intercept = clf.intercept_[0]\n",
    "        proba = 1 / (1 + np.exp(-(np.dot(Xc[test_idx], beta_post) + intercept)))\n",
    "        scores.append(log_loss(y[test_idx], proba, labels=[0,1]))\n",
    "    mean_score = np.mean(scores)\n",
    "    print('lam', lam, 'cv_log_loss', mean_score)\n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_lam = lam\n",
    "        best_beta = beta_post\n",
    "\n",
    "if best_lam is None:\n",
    "    raise ValueError('No lambda selected; likely all CV folds lacked positive examples')\n",
    "\n",
    "print('Selected lambda:', best_lam)\n",
    "\n",
    "# Refit on full data and compute probabilities\n",
    "clf_full = LogisticRegression(penalty='l2', C=1/best_lam, solver='lbfgs', max_iter=1000)\n",
    "clf_full.fit(Xc, y)\n",
    "beta_hat_full = clf_full.coef_.ravel()\n",
    "beta_post_full = (beta_hat_full + best_lam * beta_chs) / (1 + best_lam)\n",
    "intercept_full = clf_full.intercept_[0]\n",
    "\n",
    "# compute probabilities\n",
    "probs = 1 / (1 + np.exp(-(np.dot(Xc, beta_post_full) + intercept_full)))\n",
    "\n",
    "# assign probabilities back to in_sample as a new column\n",
    "probs_df = pl.DataFrame({'_idx': np.arange(df.height), 'p_failure_12_ridge': probs})\n",
    "# Note: df presently is in_sample subset; find mapping of original indexes\n",
    "# We'll materialize df with row numbers to join back\n",
    "orig = df.with_row_count('_idx')\n",
    "probs_df = orig.select('_idx').with_columns(pl.Series('p_failure_12_ridge', probs))\n",
    "\n",
    "# join back to original in_sample by generating a row number on in_sample and matching\n",
    "in_sample = in_sample.with_row_count('_idx')\n",
    "in_sample = in_sample.join(probs_df.select(['_idx','p_failure_12_ridge']), on='_idx', how='left').drop('_idx')\n",
    "\n",
    "print('Assigned p_failure_12_ridge to in_sample; non-null count=', in_sample.filter(pl.col('p_failure_12_ridge').is_not_null()).height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b25f3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1484675174.py:110: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  nimta_3m = (pl.col('nimta').shift(1).rolling_mean(window_size=3, min_periods=3).over('PERMNO'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete. in_sample rows = 2076367\n"
     ]
    }
   ],
   "source": [
    "# --- Feature engineering (adapted from original notebook) ---\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# ensure data is sorted\n",
    "data = data.sort([\"PERMNO\", \"date\"]) \n",
    "\n",
    "# cast returns\n",
    "data = data.with_columns((pl.col('DLRET').cast(pl.Float32, strict=False)).alias('DLRET'))\n",
    "data = data.with_columns((pl.col('RET').cast(pl.Float32, strict=False)).alias('RET'))\n",
    "# use DLRET when RET is null\n",
    "data = data.with_columns(\n",
    "    pl.when((pl.col('RET').is_null()) & (pl.col('DLRET').is_not_null()))\n",
    "      .then(pl.col('DLRET'))\n",
    "      .otherwise(pl.col('RET'))\n",
    "      .alias('RET')\n",
    ")\n",
    "# combine when both exist\n",
    "data = data.with_columns(\n",
    "    pl.when((pl.col('RET').is_not_null()) & (pl.col('DLRET').is_not_null()))\n",
    "      .then(((pl.col('RET')+1) * (1 + pl.col('DLRET'))) - 1)\n",
    "      .otherwise(pl.col('RET'))\n",
    "      .alias('RET')\n",
    ")\n",
    "\n",
    "# price absolute and log price\n",
    "if 'PRC' in data.columns:\n",
    "    data = data.with_columns(pl.col('PRC').abs().alias('PRC'))\n",
    "    data = data.with_columns(pl.col('PRC').log().alias('log_prc'))\n",
    "\n",
    "# book equity (beq)\n",
    "if all(c in data.columns for c in ['seqq','txditcq','pstkrq']):\n",
    "    data = data.with_columns(\n",
    "        (((pl.col('seqq') + pl.col('txditcq').fill_null(0) - pl.col('pstkrq').fill_null(0))).shift(2).over('PERMNO')).alias('beq')\n",
    "    )\n",
    "    # replace negative beq with tiny positive\n",
    "    data = data.with_columns(\n",
    "        (pl.when(pl.col('beq') < 0).then(pl.lit(1/1_000_000)).otherwise(pl.col('beq'))).alias('beq')\n",
    "    )\n",
    "\n",
    "# market equity\n",
    "if all(c in data.columns for c in ['PRC','SHROUT']):\n",
    "    data = data.with_columns((pl.col('PRC') * (pl.col('SHROUT')/1000)).alias('meq'))\n",
    "# mbq\n",
    "if all(c in data.columns for c in ['meq','beq']):\n",
    "    data = data.with_columns((pl.col('meq') / pl.col('beq')).alias('mbq'))\n",
    "# total assets adjusted\n",
    "if 'atq' in data.columns:\n",
    "    data = data.with_columns((((pl.col('atq')).shift(2).over('PERMNO')) + (0.1 * (pl.col('meq') - pl.col('beq')))).alias('taq_adjusted'))\n",
    "\n",
    "# rsize\n",
    "if all(c in data.columns for c in ['meq','totval']):\n",
    "    data = data.with_columns(((pl.col('meq')/(pl.col('totval')/1000)).log()).alias('rsize'))\n",
    "# exret\n",
    "if all(c in data.columns for c in ['RET','sprtrn']):\n",
    "    data = data.with_columns((((1 + pl.col('RET')).log()) - ((1 + pl.col('sprtrn')).log())).alias('exret'))\n",
    "\n",
    "# nimta, tlmta, cshmta, nita, tlta\n",
    "if all(c in data.columns for c in ['niq','meq','ltq','cheq','taq_adjusted']):\n",
    "    data = data.sort([\"PERMNO\", \"date\"]) \n",
    "    data = data.with_columns((((pl.col('niq')).shift(2).over('PERMNO'))/(pl.col('meq')+((pl.col('ltq')).shift(2).over('PERMNO')))).alias('nimta'))\n",
    "\n",
    "    data = data.with_columns(((pl.col('ltq')).shift(2).over('PERMNO')/ (pl.col('meq') + (pl.col('ltq')).shift(2).over('PERMNO'))).alias('tlmta'))\n",
    "\n",
    "    data = data.with_columns(((pl.col('cheq')).shift(2).over('PERMNO')/ (pl.col('meq') + (pl.col('ltq')).shift(2).over('PERMNO'))).alias('cshmta'))\n",
    "\n",
    "    data = data.with_columns((((pl.col('niq')).shift(2).over('PERMNO'))/((pl.col('taq_adjusted')))).alias('nita'))\n",
    "\n",
    "    data = data.with_columns((((pl.col('ltq')).shift(2).over('PERMNO'))/((pl.col('taq_adjusted')))).alias('tlta'))\n",
    "\n",
    "    # fill in nita and tlta with cross-sectional means by date\n",
    "    data = data.with_columns([\n",
    "        pl.col(\"nita\").fill_null(pl.col(\"nita\").mean().over(\"date\")).alias(\"nita\"),\n",
    "        pl.col(\"tlta\").fill_null(pl.col(\"tlta\").mean().over(\"date\")).alias(\"tlta\")\n",
    "    ])\n",
    "\n",
    "# --- sigma from daily CRSP ---\n",
    "crsp_daily = pl.read_csv(f\"{BOQ_DIR}/crsp_daily.csv\", infer_schema_length=10000, schema_overrides={'CUSIP': pl.Utf8, 'PERMNO': pl.Int64, 'date': pl.Utf8})\n",
    "crsp_daily = crsp_daily.with_columns((pl.col('PRC').abs()).alias('PRC'))\n",
    "if 'RET' in crsp_daily.columns:\n",
    "    crsp_daily = crsp_daily.with_columns(pl.col('RET').cast(pl.Float32, strict=False))\n",
    "if 'date' in crsp_daily.columns:\n",
    "    crsp_daily = crsp_daily.with_columns(pl.col('date').str.strptime(pl.Date, format=\"%Y-%m-%d\").alias('date'))\n",
    "crsp_daily = crsp_daily.sort(['PERMNO','date'])\n",
    "if 'RET' in crsp_daily.columns:\n",
    "    crsp_daily = crsp_daily.with_columns((pl.col('RET').rolling_std(63).over('PERMNO')).alias('std_3mo'))\n",
    "    crsp_daily = crsp_daily.with_columns((pl.col('std_3mo') * (252**0.5)).alias('sigma_ann'))\n",
    "if 'date' in crsp_daily.columns:\n",
    "    crsp_daily = crsp_daily.with_columns(pl.col('date').dt.month_end().alias('date'))\n",
    "\n",
    "sigma_m = (crsp_daily.group_by([\"PERMNO\",\"date\"]).agg(pl.col('sigma_ann').drop_nulls().last().alias('sigma')))\n",
    "# fill remaining nulls with same-month cross sectional mean\n",
    "sigma_cs = sigma_m.group_by('date').agg(pl.col('sigma').mean().alias('sigma_cs_mean'))\n",
    "sigma_m = sigma_m.join(sigma_cs, on='date', how='left').with_columns(pl.coalesce([pl.col('sigma'), pl.col('sigma_cs_mean')]).alias('sigma')).drop('sigma_cs_mean')\n",
    "\n",
    "# join sigma\n",
    "if all(c in sigma_m.columns for c in ['PERMNO','date']):\n",
    "    data = data.join(sigma_m, on=['PERMNO','date'], how='left')\n",
    "\n",
    "# --- nimtaavg and exretavg (EWMA) ---\n",
    "phi = 0.5 ** (1/3)\n",
    "\n",
    "# exretavg\n",
    "if 'exret' in data.columns:\n",
    "    ex_terms = [pl.col('exret').shift(i).over('PERMNO') * (phi ** (i - 1)) for i in range(1, 13)]\n",
    "    data = data.with_columns((((1 - phi) / (1 - phi**12)) * sum(ex_terms)).alias('exretavg'))\n",
    "\n",
    "# nimtaavg: build 3-month block averages\n",
    "if 'nimta' in data.columns:\n",
    "    nimta_3m = (pl.col('nimta').shift(1).rolling_mean(window_size=3, min_periods=3).over('PERMNO'))\n",
    "    q1 = nimta_3m\n",
    "    q2 = nimta_3m.shift(3)\n",
    "    q3 = nimta_3m.shift(6)\n",
    "    q4 = nimta_3m.shift(9)\n",
    "\n",
    "    data = data.with_columns(( ((1 - phi**3) / (1 - phi**12)) * (q1 + (phi**3)*q2 + (phi**6)*q3 + (phi**9)*q4) ).alias('nimtaavg'))\n",
    "\n",
    "# ensure finite\n",
    "if 'exretavg' in data.columns:\n",
    "    data = data.with_columns(\n",
    "        pl.when(pl.col('exretavg').is_finite()).then(pl.col('exretavg')).otherwise(None).alias('exretavg')\n",
    "    )\n",
    "if 'nimtaavg' in data.columns:\n",
    "    data = data.with_columns(\n",
    "        pl.when(pl.col('nimtaavg').is_finite()).then(pl.col('nimtaavg')).otherwise(None).alias('nimtaavg')\n",
    "    )\n",
    "\n",
    "# --- winsorize selected variables and create *_w columns ---\n",
    "vars_to_winsor = [\"nita\",\"tlta\",\"exret\",\"sigma\",\"rsize\",\"log_prc\",\"nimtaavg\",\"exretavg\",\"mbq\",\"tlmta\",\"cshmta\"]\n",
    "# clean infinities\n",
    "for c in vars_to_winsor:\n",
    "    if c in data.columns:\n",
    "        data = data.with_columns((pl.when(pl.col(c).is_infinite()).then(None).otherwise(pl.col(c)).alias(c)))\n",
    "\n",
    "# compute pooled 5%/95% cutoffs only for existing columns\n",
    "existing_vars = [c for c in vars_to_winsor if c in data.columns]\n",
    "lo_exprs = [pl.col(c).quantile(0.05, interpolation=\"lower\").alias(f\"{c}_lo\") for c in existing_vars]\n",
    "hi_exprs = [pl.col(c).quantile(0.95, interpolation=\"higher\").alias(f\"{c}_hi\") for c in existing_vars]\n",
    "if lo_exprs or hi_exprs:\n",
    "    qdf = data.select(*lo_exprs, *hi_exprs).to_dicts()[0]\n",
    "else:\n",
    "    qdf = {}\n",
    "for c in existing_vars:\n",
    "    lo = qdf.get(f\"{c}_lo\", None)\n",
    "    hi = qdf.get(f\"{c}_hi\", None)\n",
    "    if lo is not None and hi is not None:\n",
    "        data = data.with_columns(pl.when(pl.col(c) < lo).then(lo).when(pl.col(c) > hi).then(hi).otherwise(pl.col(c)).alias(f\"{c}_w\"))\n",
    "\n",
    "# log price already created above as 'log_prc' if PRC existed; ensure we've winsorized it if needed (already handled)\n",
    "\n",
    "# --- Define failure codes and failure flag ---\n",
    "failure_codes=[591,574]\n",
    "if 'DLSTCD' in data.columns:\n",
    "    data = data.with_columns(pl.col('DLSTCD').cast(pl.Int64, strict=False))\n",
    "    data = data.with_columns(pl.col('DLSTCD').is_in(failure_codes).fill_null(False).cast(pl.Int8).alias('failure'))\n",
    "else:\n",
    "    data = data.with_columns(pl.lit(0).alias('failure'))\n",
    "\n",
    "# --- in_sample (1965-01 to 2003-12) and failure_12 target ---\n",
    "start = pl.date(1965,1,1)\n",
    "end = pl.date(2003,12,31)\n",
    "\n",
    "in_sample = data.filter((pl.col('date') >= start) & (pl.col('date') <= end))\n",
    "# y_t = 1 if failure occurs any time in next 12 months (exclude month t)\n",
    "if 'failure' in in_sample.columns:\n",
    "    in_sample = in_sample.sort('PERMNO','date')\n",
    "    in_sample = in_sample.with_columns(pl.concat_list([pl.col('failure').shift(-i) for i in range(1,13)]).list.max().alias('failure_12'))\n",
    "else:\n",
    "    in_sample = in_sample.with_columns(pl.lit(0).alias('failure_12'))\n",
    "\n",
    "print('Feature engineering complete. in_sample rows =', in_sample.height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f4a2dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CSV reads with schema overrides...\n",
      "crsp_pricing.csv loaded rows= 4709982 cols= 12\n",
      "crsp_delisting.csv loaded rows= 4709982 cols= 11\n",
      "crsp_daily.csv loaded rows= 97495634 cols= 5\n",
      "CSV read tests complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Quick CSV read tests with schema_overrides to avoid dtype inference errors ---\n",
    "import polars as pl\n",
    "\n",
    "print('Testing CSV reads with schema overrides...')\n",
    "\n",
    "schema_overrides = {\n",
    "    'CUSIP': pl.Utf8,\n",
    "    'PERMNO': pl.Int64,\n",
    "    'date': pl.Utf8,\n",
    "}\n",
    "\n",
    "for fname in ['crsp_pricing.csv', 'crsp_delisting.csv', 'crsp_daily.csv']:\n",
    "    path = f\"{BOQ_DIR}/{fname}\"\n",
    "    try:\n",
    "        df = pl.read_csv(path, infer_schema_length=10000, schema_overrides=schema_overrides)\n",
    "        print(fname, 'loaded rows=', df.shape[0], 'cols=', df.shape[1])\n",
    "    except Exception as e:\n",
    "        print('Error reading', fname, type(e), e)\n",
    "\n",
    "print('CSV read tests complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2861866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in in_sample selection: 2076367\n",
      "Nulls in nimtaavg_w: 2076356\n",
      "Nulls in tlmta_w: 1550565\n",
      "Nulls in rsize_w: 81552\n",
      "Nulls in exretavg_w: 391220\n",
      "Nulls in sigma_w: 18193\n",
      "Nulls in log_prc_w: 79602\n",
      "Nulls in mbq_w: 1524402\n",
      "Nulls in cshmta_w: 1556088\n",
      "Nulls in failure_12: 1\n",
      "Rows after drop_nulls: 8\n",
      "Class counts:\n",
      "shape: (1, 2)\n",
      "┌────────────┬───────┐\n",
      "│ failure_12 ┆ count │\n",
      "│ ---        ┆ ---   │\n",
      "│ i8         ┆ u32   │\n",
      "╞════════════╪═══════╡\n",
      "│ 0          ┆ 8     │\n",
      "└────────────┴───────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1309268897.py:16: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  counts = df2.group_by('failure_12').agg(pl.count()).sort('failure_12')\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics: inspect training data used for fit\n",
    "predictors = ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "df = in_sample.select(['failure_12'] + predictors)\n",
    "print('Total rows in in_sample selection:', df.height)\n",
    "# null counts per predictor\n",
    "for p in predictors + ['failure_12']:\n",
    "    if p in df.columns:\n",
    "        nulls = df.filter(pl.col(p).is_null()).height\n",
    "        print(f'Nulls in {p}:', nulls)\n",
    "\n",
    "df2 = df.drop_nulls()\n",
    "print('Rows after drop_nulls:', df2.height)\n",
    "# class distribution\n",
    "if 'failure_12' in df2.columns and df2.height > 0:\n",
    "    print('Class counts:')\n",
    "    counts = df2.group_by('failure_12').agg(pl.count()).sort('failure_12')\n",
    "    print(counts)\n",
    "else:\n",
    "    print('No rows to show class counts or failure_12 missing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8734904d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ColumnNotFoundError",
     "evalue": "unable to find column \"p_failure_12_ridge\"; valid columns: [\"PERMNO\", \"date\", \"SHRCD\", \"EXCHCD\", \"CUSIP\", \"BIDLO\", \"ASKHI\", \"PRC\", \"VOL\", \"RET\", \"SHROUT\", \"RETX\", \"NCUSIP\", \"TICKER\", \"DLAMT\", \"DLPDT\", \"DLSTCD\", \"DLRETX\", \"DLPRC\", \"DLRET\", \"costat\", \"curcdq\", \"datafmt\", \"indfmt\", \"consol\", \"gvkey\", \"cusip\", \"atq\", \"cheq\", \"ltq\", \"niq\", \"pstkrq\", \"seqq\", \"txditcq\", \"LINKTYPE\", \"LPERMCO\", \"LINKDT\", \"LINKENDDT\", \"tic\", \"caldt\", \"totval\", \"totcnt\", \"usdval\", \"spindx\", \"sprtrn\", \"caldt_date\", \"log_prc\", \"beq\", \"meq\", \"mbq\", \"taq_adjusted\", \"rsize\", \"exret\", \"nimta\", \"tlmta\", \"cshmta\", \"nita\", \"tlta\", \"sigma\", \"exretavg\", \"nimtaavg\", \"nita_w\", \"tlta_w\", \"exret_w\", \"sigma_w\", \"rsize_w\", \"log_prc_w\", \"nimtaavg_w\", \"exretavg_w\", \"mbq_w\", \"tlmta_w\", \"cshmta_w\", \"failure\", \"failure_12\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mColumnNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Prepare evaluation frame: use rows where both CHS predictors and p_failure_12_ridge exist\u001b[39;00m\n\u001b[32m     11\u001b[39m predictors = [\u001b[33m'\u001b[39m\u001b[33mnimtaavg_w\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtlmta_w\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mrsize_w\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mexretavg_w\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33msigma_w\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mlog_prc_w\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mmbq_w\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mcshmta_w\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sel = \u001b[43min_sample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfailure_12\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mp_failure_12_ridge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictors\u001b[49m\u001b[43m)\u001b[49m.drop_nulls()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mRows available for diagnostics:\u001b[39m\u001b[33m'\u001b[39m, sel.height)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sel.height > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ACME/.venv/lib/python3.13/site-packages/polars/dataframe/frame.py:10007\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *exprs, **named_exprs)\u001b[39m\n\u001b[32m   9923\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   9924\u001b[39m \u001b[33;03mSelect columns from this DataFrame.\u001b[39;00m\n\u001b[32m   9925\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m  10000\u001b[39m \u001b[33;03m└───────────┘\u001b[39;00m\n\u001b[32m  10001\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m  10002\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m  10004\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m  10005\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10006\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m> \u001b[39m\u001b[32m10007\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10008\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ACME/.venv/lib/python3.13/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ACME/.venv/lib/python3.13/site-packages/polars/lazyframe/opt_flags.py:330\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    329\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ACME/.venv/lib/python3.13/site-packages/polars/lazyframe/frame.py:2407\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2405\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2406\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mColumnNotFoundError\u001b[39m: unable to find column \"p_failure_12_ridge\"; valid columns: [\"PERMNO\", \"date\", \"SHRCD\", \"EXCHCD\", \"CUSIP\", \"BIDLO\", \"ASKHI\", \"PRC\", \"VOL\", \"RET\", \"SHROUT\", \"RETX\", \"NCUSIP\", \"TICKER\", \"DLAMT\", \"DLPDT\", \"DLSTCD\", \"DLRETX\", \"DLPRC\", \"DLRET\", \"costat\", \"curcdq\", \"datafmt\", \"indfmt\", \"consol\", \"gvkey\", \"cusip\", \"atq\", \"cheq\", \"ltq\", \"niq\", \"pstkrq\", \"seqq\", \"txditcq\", \"LINKTYPE\", \"LPERMCO\", \"LINKDT\", \"LINKENDDT\", \"tic\", \"caldt\", \"totval\", \"totcnt\", \"usdval\", \"spindx\", \"sprtrn\", \"caldt_date\", \"log_prc\", \"beq\", \"meq\", \"mbq\", \"taq_adjusted\", \"rsize\", \"exret\", \"nimta\", \"tlmta\", \"cshmta\", \"nita\", \"tlta\", \"sigma\", \"exretavg\", \"nimtaavg\", \"nita_w\", \"tlta_w\", \"exret_w\", \"sigma_w\", \"rsize_w\", \"log_prc_w\", \"nimtaavg_w\", \"exretavg_w\", \"mbq_w\", \"tlmta_w\", \"cshmta_w\", \"failure\", \"failure_12\"]"
     ]
    }
   ],
   "source": [
    "# Diagnostics: compare CHS (hard-coded) vs p_failure_12_ridge\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score\n",
    "\n",
    "# define CHS 12-month coefficients & intercept (approx) - replace if different in original notebook\n",
    "chs_beta = np.array([-20.26, 1.42, -0.55, -0.999, 0.317, 0.953, 0.075, -0.097])\n",
    "# predictor order used earlier: ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "chs_intercept = -3.0  # placeholder; change if the original CHS intercept exists\n",
    "\n",
    "# Prepare evaluation frame: use rows where both CHS predictors and p_failure_12_ridge exist\n",
    "predictors = ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "sel = in_sample.select(['failure_12','p_failure_12_ridge'] + predictors).drop_nulls()\n",
    "print('Rows available for diagnostics:', sel.height)\n",
    "if sel.height > 0:\n",
    "    X = np.column_stack([np.asarray(sel[p].to_numpy()) for p in predictors])\n",
    "    y = np.asarray(sel['failure_12'].to_numpy())\n",
    "    # CHS probabilities\n",
    "    p_chs = 1 / (1 + np.exp(-(np.dot(X, chs_beta) + chs_intercept)))\n",
    "    p_ridge = np.asarray(sel['p_failure_12_ridge'].to_numpy())\n",
    "    # metrics\n",
    "    print('CHS log_loss:', log_loss(y, p_chs, labels=[0,1]))\n",
    "    print('Ridge-prior log_loss:', log_loss(y, p_ridge, labels=[0,1]))\n",
    "    print('CHS Brier:', brier_score_loss(y, p_chs))\n",
    "    print('Ridge-prior Brier:', brier_score_loss(y, p_ridge))\n",
    "    try:\n",
    "        print('CHS AUC:', roc_auc_score(y, p_chs))\n",
    "        print('Ridge-prior AUC:', roc_auc_score(y, p_ridge))\n",
    "    except Exception as e:\n",
    "        print('AUC error (maybe single class):', e)\n",
    "else:\n",
    "    print('No rows available for diagnostics. Ensure predictors and p_failure_12_ridge are present.')\n",
    "\n",
    "\n",
    "# 1-month prior-anchored fit (repeat of 12-month flow but with 1-month target)\n",
    "# CHS 1-month coefficients (placeholder values - replace with actual CHS 1-month betas if known)\n",
    "beta_chs_1m = np.array([-1.327, 4.743, -0.055, -0.999, 0.317, 0.953, 0.032, -0.097])\n",
    "\n",
    "# Build df for 1-month: assume in_sample has 'failure_1' or create it if not present\n",
    "if 'failure_1' not in in_sample.columns:\n",
    "    # create failure_1 by shifting the 'failure' column forward 1 month\n",
    "    if 'failure' in in_sample.columns:\n",
    "        in_sample = in_sample.with_row_count('_idx')\n",
    "        temp = in_sample.select(['_idx','PERMNO','date','failure'])\n",
    "        # compute failure_1 (any failure in next 1 month)\n",
    "        in_sample = in_sample.with_columns(pl.concat_list([pl.col('failure').shift(-i) for i in [1]]).list.max().alias('failure_1')).drop('_idx')\n",
    "\n",
    "# Prepare df\n",
    "predictors = ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "df1 = in_sample.select(['failure_1'] + predictors)\n",
    "# median-impute like 12-month cell\n",
    "for p in predictors:\n",
    "    if p in df1.columns:\n",
    "        median_val = df1.select(pl.col(p)).drop_nulls().median().to_numpy()[0] if df1.select(pl.col(p)).drop_nulls().height>0 else 0.0\n",
    "        df1 = df1.with_columns(pl.col(p).fill_null(median_val))\n",
    "if 'failure_1' in df1.columns:\n",
    "    df1 = df1.filter(pl.col('failure_1').is_not_null())\n",
    "\n",
    "if df1.height == 0:\n",
    "    print('No rows for 1-month fit')\n",
    "else:\n",
    "    y1 = np.asarray(df1['failure_1'].to_numpy())\n",
    "    X1 = np.column_stack([np.asarray(df1[p].to_numpy()) for p in predictors])\n",
    "    X1c = X1 - X1.mean(axis=0)\n",
    "    lambdas = np.logspace(-2,2,7)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    from sklearn.metrics import log_loss\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    best_score = np.inf\n",
    "    best_lam = None\n",
    "    for lam in lambdas:\n",
    "        scores = []\n",
    "        for train_idx, test_idx in tscv.split(X1c):\n",
    "            if len(np.unique(y1[train_idx])) < 2:\n",
    "                scores.append(np.inf)\n",
    "                continue\n",
    "            clf = LogisticRegression(penalty='l2', C=1/lam, solver='lbfgs', max_iter=1000)\n",
    "            clf.fit(X1c[train_idx], y1[train_idx])\n",
    "            beta_hat = clf.coef_.ravel()\n",
    "            beta_post = (beta_hat + lam * beta_chs_1m) / (1 + lam)\n",
    "            intercept = clf.intercept_[0]\n",
    "            proba = 1 / (1 + np.exp(-(np.dot(X1c[test_idx], beta_post) + intercept)))\n",
    "            scores.append(log_loss(y1[test_idx], proba, labels=[0,1]))\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_lam = lam\n",
    "    print('1-month selected lambda:', best_lam)\n",
    "    # Refit full model\n",
    "    clf_full = LogisticRegression(penalty='l2', C=1/best_lam, solver='lbfgs', max_iter=1000)\n",
    "    clf_full.fit(X1c, y1)\n",
    "    beta_hat_full = clf_full.coef_.ravel()\n",
    "    beta_post_full = (beta_hat_full + best_lam * beta_chs_1m) / (1 + best_lam)\n",
    "    intercept_full = clf_full.intercept_[0]\n",
    "    probs1 = 1 / (1 + np.exp(-(np.dot(X1c, beta_post_full) + intercept_full)))\n",
    "    # assign back to in_sample\n",
    "    out_df = df1.with_row_count('_idx').select(['_idx']).with_columns(pl.Series('p_failure_1_ridge', probs1))\n",
    "    in_sample = in_sample.with_row_count('_idx')\n",
    "    in_sample = in_sample.join(out_df.select(['_idx','p_failure_1_ridge']), on='_idx', how='left').drop('_idx')\n",
    "    print('Assigned p_failure_1_ridge to in_sample; non-null count=', in_sample.filter(pl.col('p_failure_1_ridge').is_not_null()).height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be742c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in imputed evaluation set: 2076366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_18029/1045300710.py:28: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  pr = in_sample.select(['p_failure_12_ridge']).with_row_count('_idx')\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_18029/1045300710.py:29: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  eval_idx = eval_df.with_row_count('_idx').select('_idx')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHS log_loss: 5.750191505427849\n",
      "Ridge-prior log_loss: 0.01626154084885955\n",
      "CHS Brier: 0.9893517843254178\n",
      "Ridge-prior Brier: 0.0026682013470842422\n",
      "CHS AUC: 0.4586020999892001\n",
      "Ridge-prior AUC: 0.8449109882865116\n",
      "Saved model betas/intercepts to /Users/danilomcmaster/Desktop/ACME/BoQ Files/prior_anchored_model_results.json\n"
     ]
    }
   ],
   "source": [
    "# Robust diagnostics on median-imputed evaluation set and save final betas\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, brier_score_loss, roc_auc_score\n",
    "\n",
    "predictors = ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "# build eval_df by median-imputing predictors on in_sample\n",
    "eval_df = in_sample.select(['failure_12'] + predictors).clone()\n",
    "for p in predictors:\n",
    "    if p in eval_df.columns:\n",
    "        nonnull = eval_df.select(pl.col(p)).drop_nulls()\n",
    "        median_val = nonnull.median().to_numpy()[0] if nonnull.height>0 else 0.0\n",
    "        eval_df = eval_df.with_columns(pl.col(p).fill_null(median_val))\n",
    "# drop rows without target\n",
    "eval_df = eval_df.filter(pl.col('failure_12').is_not_null())\n",
    "print('Rows in imputed evaluation set:', eval_df.height)\n",
    "\n",
    "if eval_df.height > 0:\n",
    "    X_eval = np.column_stack([np.asarray(eval_df[p].to_numpy()) for p in predictors])\n",
    "    y_eval = np.asarray(eval_df['failure_12'].to_numpy())\n",
    "    # CHS using same predictor order and placeholder intercept (replace if you have true intercept)\n",
    "    chs_beta = chs_beta if 'chs_beta' in globals() else np.array([-20.26, 1.42, -0.55, -0.999, 0.317, 0.953, 0.075, -0.097])\n",
    "    chs_intercept = chs_intercept if 'chs_intercept' in globals() else -3.0\n",
    "    p_chs_eval = 1 / (1 + np.exp(-(np.dot(X_eval, chs_beta) + chs_intercept)))\n",
    "    # ridge predictions (use p_failure_12_ridge column if present else recompute)\n",
    "    if 'p_failure_12_ridge' in in_sample.columns:\n",
    "        # build p_ridge aligned with eval_df by re-deriving from in_sample\n",
    "        pr = in_sample.select(['p_failure_12_ridge']).with_row_count('_idx')\n",
    "        eval_idx = eval_df.with_row_count('_idx').select('_idx')\n",
    "        merged = eval_idx.join(pr, on='_idx', how='left')\n",
    "        p_ridge_eval = np.asarray(merged['p_failure_12_ridge'].fill_null(0).to_numpy())\n",
    "    else:\n",
    "        p_ridge_eval = np.zeros(X_eval.shape[0])\n",
    "\n",
    "    print('CHS log_loss:', log_loss(y_eval, p_chs_eval, labels=[0,1]))\n",
    "    print('Ridge-prior log_loss:', log_loss(y_eval, p_ridge_eval, labels=[0,1]))\n",
    "    print('CHS Brier:', brier_score_loss(y_eval, p_chs_eval))\n",
    "    print('Ridge-prior Brier:', brier_score_loss(y_eval, p_ridge_eval))\n",
    "    try:\n",
    "        print('CHS AUC:', roc_auc_score(y_eval, p_chs_eval))\n",
    "        print('Ridge-prior AUC:', roc_auc_score(y_eval, p_ridge_eval))\n",
    "    except Exception as e:\n",
    "        print('AUC error (possibly single-class):', e)\n",
    "\n",
    "    # Save final betas/intercepts for 12m and 1m\n",
    "    results = {}\n",
    "    # 12-month\n",
    "    results['12m'] = {'beta_post': beta_post_full.tolist() if 'beta_post_full' in globals() else None,\n",
    "                      'intercept': float(intercept_full) if 'intercept_full' in globals() else None,\n",
    "                      'selected_lambda': float(best_lam) if 'best_lam' in globals() else None}\n",
    "    # 1-month: try to get from variables created in 1-month cell\n",
    "    results['1m'] = {'beta_post': beta_post_full.tolist() if 'beta_post_full' in globals() else None,\n",
    "                     'intercept': float(intercept_full) if 'intercept_full' in globals() else None,\n",
    "                     'selected_lambda': float(best_lam) if 'best_lam' in globals() else None}\n",
    "\n",
    "    out_path = f\"{BOQ_DIR}/prior_anchored_model_results.json\"\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print('Saved model betas/intercepts to', out_path)\n",
    "else:\n",
    "    print('No rows in imputed evaluation set; cannot compute diagnostics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c56ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added p_failure_12_chs and p_failure_1_chs to in_sample; non-null counts: 2076367 2076367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_18029/3619234926.py:33: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  in_sample = in_sample.with_row_count('_idx')\n"
     ]
    }
   ],
   "source": [
    "# Apply CHS beta_actuals exactly like the original notebook: compute p_failure_12_chs and p_failure_1_chs\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# predictor order\n",
    "predictors = ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "# CHS betas (12-month) — replace these values with the exact CHS betas from the original if different\n",
    "chs_betas_12 = np.array([-20.26, 1.42, -0.55, -0.999, 0.317, 0.953, 0.075, -0.097])\n",
    "chs_intercept_12 = -3.0  # placeholder; use original notebook's intercept if provided\n",
    "\n",
    "# CHS betas (1-month) — replace with exact values if known\n",
    "chs_betas_1 = np.array([-1.327, 4.743, -0.055, -0.999, 0.317, 0.953, 0.032, -0.097])\n",
    "chs_intercept_1 = -1.0  # placeholder\n",
    "\n",
    "# build a median-imputed frame consistent with how we evaluated earlier\n",
    "df_chs = in_sample.select(predictors).clone()\n",
    "for p in predictors:\n",
    "    if p in df_chs.columns:\n",
    "        nonnull = df_chs.select(pl.col(p)).drop_nulls()\n",
    "        median_val = nonnull.median().to_numpy()[0] if nonnull.height>0 else 0.0\n",
    "        df_chs = df_chs.with_columns(pl.col(p).fill_null(median_val))\n",
    "\n",
    "# compute CHS linear scores and probs\n",
    "X_chs = np.column_stack([np.asarray(df_chs[p].to_numpy()) for p in predictors])\n",
    "linear_12 = np.dot(X_chs, chs_betas_12) + chs_intercept_12\n",
    "p_chs_12 = 1 / (1 + np.exp(-linear_12))\n",
    "linear_1 = np.dot(X_chs, chs_betas_1) + chs_intercept_1\n",
    "p_chs_1 = 1 / (1 + np.exp(-linear_1))\n",
    "\n",
    "# attach to in_sample\n",
    "probs_df = pl.DataFrame({'_idx': np.arange(df_chs.height), 'p_failure_12_chs': p_chs_12, 'p_failure_1_chs': p_chs_1})\n",
    "# join by row number\n",
    "in_sample = in_sample.with_row_count('_idx')\n",
    "in_sample = in_sample.join(probs_df, on='_idx', how='left').drop('_idx')\n",
    "\n",
    "print('Added p_failure_12_chs and p_failure_1_chs to in_sample; non-null counts:', in_sample.filter(pl.col('p_failure_12_chs').is_not_null()).height, in_sample.filter(pl.col('p_failure_1_chs').is_not_null()).height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_failure_12_ridge non-null count= 2076366\n",
      "p_failure_1_ridge MISSING\n",
      "p_failure_12_chs non-null count= 2076367\n",
      "p_failure_1_chs non-null count= 2076367\n",
      "\n",
      "Saved JSON summary:\n",
      "{\n",
      "  \"12m\": {\n",
      "    \"beta_post\": [\n",
      "      -0.20059148440144553,\n",
      "      1.300427984454351,\n",
      "      0.05748108530145743,\n",
      "      -0.5132354103883862,\n",
      "      1.963681748107379,\n",
      "      -0.6957108437213182,\n",
      "      0.03055680641068469,\n",
      "      -0.26562084234122807\n",
      "    ],\n",
      "    \"intercept\": -6.72553280751739,\n",
      "    \"selected_lambda\": 0.01\n",
      "  },\n",
      "  \"1m\": {\n",
      "    \"beta_post\": [\n",
      "      -0.20059148440144553,\n",
      "      1.300427984454351,\n",
      "      0.05748108530145743,\n",
      "      -0.5132354103883862,\n",
      "      1.963681748107379,\n",
      "      -0.6957108437213182,\n",
      "      0.03055680641068469,\n",
      "      -0.26562084234122807\n",
      "    ],\n",
      "    \"intercept\": -6.72553280751739,\n",
      "    \"selected_lambda\": 0.01\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Verification cell: print non-null counts for predictions and show saved JSON\n",
    "import json\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "cols = ['p_failure_12_ridge','p_failure_1_ridge','p_failure_12_chs','p_failure_1_chs']\n",
    "for c in cols:\n",
    "    if c in in_sample.columns:\n",
    "        print(c, 'non-null count=', in_sample.filter(pl.col(c).is_not_null()).height)\n",
    "    else:\n",
    "        print(c, 'MISSING')\n",
    "\n",
    "pjson = f\"{BOQ_DIR}/prior_anchored_model_results.json\"\n",
    "if os.path.exists(pjson):\n",
    "    with open(pjson) as f:\n",
    "        j = json.load(f)\n",
    "    print('\\nSaved JSON summary:')\n",
    "    print(json.dumps(j, indent=2))\n",
    "else:\n",
    "    print('\\nNo JSON file found at', pjson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Portfolio Formation: load FF5 and convert date to month-end ---\n",
    "import polars as pl\n",
    "# read ff5 and convert 'date' like '196307' to the month's last calendar day\n",
    "ff5 = pl.read_csv(f\"{BOQ_DIR}/ff5.csv\")\n",
    "ff5 = ff5.with_columns(\n",
    "    (\n",
    "        pl.concat_str([pl.col(\"date\").cast(pl.Utf8), pl.lit(\"01\")])  # \"19630701\"\n",
    "        .str.strptime(pl.Date, format=\"%Y%m%d\")                        # 1963-07-01\n",
    "        .dt.offset_by(\"1mo\")                                        # 1963-08-01\n",
    "        .dt.replace(day=1)                                          # 1963-08-01 (idempotent)\n",
    "        - pl.duration(days=1)                                       # 1963-07-31\n",
    "    ).alias(\"date\")\n",
    ")\n",
    "\n",
    "print('Loaded ff5 with rows=', ff5.height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Monthly Rebalance (Equal Weight) + plotting ---\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# create decile bins by p_failure (requires p_failure present in in_sample)\n",
    "bin_names=[str(i) for i in range(10)]\n",
    "decile1 = in_sample.filter(pl.col('p_failure').is_not_null())\n",
    "decile1 = decile1.with_columns((pl.col('p_failure').qcut(10, labels=bin_names)).alias('bin'))\n",
    "# aggregate mean returns per bin & pivot\n",
    "rets = decile1.group_by(['bin','date']).agg(pl.col('RET').mean()).sort('bin','date')\n",
    "rets = rets.pivot(index='date', on='bin', values='RET').sort('date')\n",
    "# cumulative series and spread\n",
    "for bin in bin_names:\n",
    "    rets = rets.with_columns((pl.col(f'{bin}').cum_sum()).alias(f'{bin}_cum'))\n",
    "rets = rets.with_columns((pl.col('0_cum') - pl.col('9_cum')).alias('spread_cum'))\n",
    "rets = rets.with_columns((pl.col('0') - pl.col('9')).alias('spread'))\n",
    "\n",
    "# rolling vol metrics (lookback months)\n",
    "lookback = 36\n",
    "bin_w_spread = bin_names + ['spread']\n",
    "rets = rets.with_columns([\n",
    "    pl.col(b).rolling_std(window_size=lookback, min_periods=lookback).alias(f\"{b}_vol_m\")\n",
    "    for b in bin_w_spread\n",
    "])\n",
    "rets = rets.with_columns([\n",
    "    (pl.col(b).rolling_std(window_size=lookback, min_periods=lookback) * (12 ** 0.5)).alias(f\"{b}_vol_ann\")\n",
    "    for b in bin_w_spread\n",
    "])\n",
    "# vol-scaled returns (annualized)\n",
    "rets = rets.with_columns([\n",
    "    pl.when(pl.col(f\"{b}_vol_ann\") > 0).then(pl.col(b) / pl.col(f\"{b}_vol_ann\")).otherwise(None).alias(f\"{b}_vol_scaled_ann\")\n",
    "    for b in bin_w_spread\n",
    "])\n",
    "# cumulative annualized vol-scaled returns\n",
    "for b in bin_w_spread:\n",
    "    rets = rets.with_columns(pl.col(f'{b}_vol_scaled_ann').cum_sum().alias(f'{b}_vol_scaled_ann_cum'))\n",
    "\n",
    "# plot cumulative annualized vol-scaled returns\n",
    "plt.figure(figsize=(12,6))\n",
    "for b in bin_w_spread:\n",
    "    try:\n",
    "        sub = rets.select([pl.col('date'), pl.col(f'{b}_vol_scaled_ann_cum')]).to_pandas().dropna()\n",
    "        x = sub['date']\n",
    "        y = sub[f'{b}_vol_scaled_ann_cum']\n",
    "        if b == 'spread':\n",
    "            plt.plot(x, y, label=f'{b}', linewidth=2, color='black')\n",
    "        else:\n",
    "            plt.plot(x, y, label=f'{b}', linewidth=1)\n",
    "    except Exception:\n",
    "        pass\n",
    "plt.title('Cumulative Annualized Vol-Scaled Portfolios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Scaled Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Sharpe ratios per bin\n",
    "rets_pd = rets.to_pandas().set_index('date')\n",
    "bin_cols = [str(i) for i in range(10)]\n",
    "rets_bins = rets_pd[bin_cols].apply(pd.to_numeric, errors='coerce')\n",
    "rf = 0.0\n",
    "excess = rets_bins - rf\n",
    "mean_ret = excess.mean()\n",
    "vol = excess.std()\n",
    "sharpe = (mean_ret / vol) * np.sqrt(12)\n",
    "print('Sharpe ratios:')\n",
    "print(sharpe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Factor regressions (FF factors) on portfolio series ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# ensure ff5 and rets exist and are pandas-ready\n",
    "ff = ff5.to_pandas().copy()\n",
    "rets_pd = rets.to_pandas().copy()\n",
    "ff['date'] = pd.to_datetime(ff['date'])\n",
    "rets_pd['date'] = pd.to_datetime(rets_pd['date'])\n",
    "ff = ff.set_index('date').sort_index()\n",
    "rets_pd = rets_pd.set_index('date').sort_index()\n",
    "factor_cols = ['Mkt-RF','SMB','HML','RMW','CMA','RF']\n",
    "ff[factor_cols] = ff[factor_cols].apply(pd.to_numeric, errors='coerce')\n",
    "if ff[factor_cols].abs().median().max() > 0.5:\n",
    "    ff[factor_cols] = ff[factor_cols] / 100.0\n",
    "M = rets_pd.join(ff[factor_cols], how='inner')\n",
    "try:\n",
    "    bin_cols = [c for c in bin_cols if c in M.columns]\n",
    "except NameError:\n",
    "    bin_cols = [c for c in rets_pd.columns if c not in factor_cols]\n",
    "M[bin_cols] = M[bin_cols].apply(pd.to_numeric, errors='coerce')\n",
    "factors = M[['Mkt-RF','SMB','HML','RMW','CMA']]\n",
    "coefs, tstats, skipped = {}, {}, {}\n",
    "for b in bin_cols:\n",
    "    tmp = pd.concat([M[[b, 'RF']], factors], axis=1)\n",
    "    tmp.columns = ['port','RF','Mkt-RF','SMB','HML','RMW','CMA']\n",
    "    tmp['y'] = tmp['port'] - tmp['RF']\n",
    "    tmp = tmp.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if tmp.shape[0] <= 6:\n",
    "        skipped[b] = f'not enough obs (n={tmp.shape[0]})'\n",
    "        continue\n",
    "    X = sm.add_constant(tmp[['Mkt-RF','SMB','HML','RMW','CMA']], has_constant='add')\n",
    "    y = tmp['y']\n",
    "    res = sm.OLS(y, X).fit()\n",
    "    coefs[b], tstats[b] = res.params, res.tvalues\n",
    "row_order = ['const','Mkt-RF','SMB','HML','RMW','CMA']\n",
    "coef_df = pd.DataFrame(coefs).reindex(row_order)\n",
    "tstat_df = pd.DataFrame(tstats).reindex(row_order)\n",
    "table = coef_df.round(4).astype(str) + ' (' + tstat_df.round(2).astype(str) + ')'\n",
    "print('Skipped portfolios:', skipped)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f4c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/verified failure_1 column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df1[p].fillna(med, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering for failure_1 non-null: 2051670\n",
      "`beta_chs_1m` not found; using placeholder values (check these)\n",
      "Selected lambda for 1-month CV: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:101: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  in_sample = in_sample.with_row_count('_idx')\n",
      "/var/folders/j4/g876sm2n2nq_xl6kzw2lvgfc0000gn/T/ipykernel_44522/1003379942.py:102: DeprecationWarning: `DataFrame.with_row_count` is deprecated; use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  out_pl2 = out_pl.with_row_count('_out_idx')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned p_failure_1_ridge to in_sample; non-null count= 2051670\n"
     ]
    }
   ],
   "source": [
    "# --- Robust diagnostics and 1-month prior-anchored fit ---\n",
    "# This cell does: safe creation of `failure_1`, median-impute predictors, run the 1-month prior-anchored ridge/logit fit,\n",
    "# and assign `p_failure_1_ridge` back to `in_sample`. Prints diagnostics and catches exceptions for quick debugging.\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "try:\n",
    "    predictors = ['nimtaavg_w','tlmta_w','rsize_w','exretavg_w','sigma_w','log_prc_w','mbq_w','cshmta_w']\n",
    "\n",
    "    # ensure in_sample exists\n",
    "    if 'in_sample' not in globals():\n",
    "        raise RuntimeError('`in_sample` not found in notebook kernel. Run earlier feature-engineering cells first.')\n",
    "\n",
    "    # 1) create failure_1 robustly per PERMNO (next-row shift within firm)\n",
    "    try:\n",
    "        in_sample = in_sample.sort(['PERMNO','date'])\n",
    "        if 'failure_1' not in in_sample.columns:\n",
    "            in_sample = in_sample.with_columns(pl.col('failure').shift(-1).over('PERMNO').alias('failure_1'))\n",
    "        print('Created/verified failure_1 column')\n",
    "    except Exception as e:\n",
    "        print('Error creating failure_1 via polars shift:', e)\n",
    "        raise\n",
    "\n",
    "    # 2) build pandas frame and median-impute predictors (safe numeric conversion)\n",
    "    available = [p for p in predictors if p in in_sample.columns]\n",
    "    if len(available) != len(predictors):\n",
    "        print('Warning: missing predictors:', sorted(list(set(predictors) - set(available))))\n",
    "\n",
    "    df1 = in_sample.select(['failure_1'] + available).to_pandas()\n",
    "    # coerce numeric and median-impute\n",
    "    for p in available:\n",
    "        df1[p] = pd.to_numeric(df1[p], errors='coerce')\n",
    "        if df1[p].notna().any():\n",
    "            med = df1[p].median()\n",
    "        else:\n",
    "            med = 0.0\n",
    "        df1[p].fillna(med, inplace=True)\n",
    "\n",
    "    # drop rows with missing failure_1\n",
    "    df1 = df1[df1['failure_1'].notna()].copy()\n",
    "    print('Rows after filtering for failure_1 non-null:', len(df1))\n",
    "    if df1.shape[0] == 0:\n",
    "        raise RuntimeError('No rows available for 1-month fit after imputation and filtering')\n",
    "\n",
    "    # 3) prepare X/y\n",
    "    X1 = df1[available].to_numpy(dtype=float)\n",
    "    y1 = df1['failure_1'].astype(int).to_numpy()\n",
    "    X1c = X1 - X1.mean(axis=0)\n",
    "\n",
    "    # 4) CHS 1-month betas fallback if not present\n",
    "    if 'beta_chs_1m' not in globals():\n",
    "        print('`beta_chs_1m` not found; using placeholder values (check these)')\n",
    "        beta_chs_1m = np.array([-1.327, 4.743, -0.055, -0.999, 0.317, 0.953, 0.032, -0.097])\n",
    "\n",
    "    if len(beta_chs_1m) != X1c.shape[1]:\n",
    "        raise ValueError(f'Length mismatch: len(beta_chs_1m)={len(beta_chs_1m)} vs X columns={X1c.shape[1]}')\n",
    "\n",
    "    # 5) cross-validate lambda (time-series) with safe checks\n",
    "    lambdas = np.logspace(-2,2,7)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    best_score = np.inf\n",
    "    best_lam = None\n",
    "\n",
    "    for lam in lambdas:\n",
    "        scores = []\n",
    "        for train_idx, test_idx in tscv.split(X1c):\n",
    "            if len(np.unique(y1[train_idx])) < 2:\n",
    "                scores.append(np.inf)\n",
    "                continue\n",
    "            clf = LogisticRegression(penalty='l2', C=1.0/lam, solver='lbfgs', max_iter=2000)\n",
    "            clf.fit(X1c[train_idx], y1[train_idx])\n",
    "            beta_hat = clf.coef_.ravel()\n",
    "            beta_post = (beta_hat + lam * beta_chs_1m) / (1.0 + lam)\n",
    "            intercept = float(clf.intercept_[0])\n",
    "            proba = 1.0 / (1.0 + np.exp(-(np.dot(X1c[test_idx], beta_post) + intercept)))\n",
    "            scores.append(log_loss(y1[test_idx], proba, labels=[0,1]))\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_lam = lam\n",
    "    print('Selected lambda for 1-month CV:', best_lam)\n",
    "\n",
    "    # 6) refit full model and assign probabilities back to in_sample\n",
    "    clf_full = LogisticRegression(penalty='l2', C=1.0/best_lam, solver='lbfgs', max_iter=2000)\n",
    "    clf_full.fit(X1c, y1)\n",
    "    beta_hat_full = clf_full.coef_.ravel()\n",
    "    beta_post_full = (beta_hat_full + best_lam * beta_chs_1m) / (1.0 + best_lam)\n",
    "    intercept_full = float(clf_full.intercept_[0])\n",
    "    probs1 = 1.0 / (1.0 + np.exp(-(np.dot(X1c, beta_post_full) + intercept_full)))\n",
    "\n",
    "    # prepare out_df with original pandas index -> convert to polars and join\n",
    "    out_pd = pd.DataFrame({'_pd_idx': df1.index, 'p_failure_1_ridge': probs1})\n",
    "    out_pl = pl.from_pandas(out_pd)\n",
    "\n",
    "    # align with in_sample using row_count index\n",
    "    in_sample = in_sample.with_row_count('_idx')\n",
    "    out_pl2 = out_pl.with_row_count('_out_idx')\n",
    "    # map pandas index to _idx by joining on where original index equals _idx\n",
    "    # create mapping: _idx values are 0..N-1 in same order as in_sample.to_pandas()\n",
    "    # compute the original pandas positions for rows we used\n",
    "    positions = df1.index.to_numpy()\n",
    "    mapping_pd = pd.DataFrame({'_pd_idx': positions, '_idx': positions})\n",
    "    mapping_pl = pl.from_pandas(mapping_pd)\n",
    "    out_pl = out_pl.join(mapping_pl, on='_pd_idx', how='left')\n",
    "    out_pl = out_pl.select(['_idx','p_failure_1_ridge']).drop_nulls()\n",
    "\n",
    "    # finally, join back\n",
    "    in_sample = in_sample.join(out_pl, on='_idx', how='left')\n",
    "    in_sample = in_sample.drop('_idx')\n",
    "\n",
    "    non_null = in_sample.filter(pl.col('p_failure_1_ridge').is_not_null()).height\n",
    "    print('Assigned p_failure_1_ridge to in_sample; non-null count=', non_null)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Exception during robust 1-month flow:')\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
